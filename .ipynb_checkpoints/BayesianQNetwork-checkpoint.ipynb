{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBQN_learn():\n",
    "    \n",
    "    def __init__(self, model, target_model, gamma, lr, batch_size, num_steps):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dbqn = model\n",
    "        self.target_dbqn = target_model\n",
    "        self.update_target()\n",
    "        \n",
    "        self.t = 0\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(lr = lr, params = self.dbqn.parameters())\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.train_freq = 1\n",
    "        self.update_freq = 100\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(10000, 0.7)\n",
    "        self.constt = 1/self.num_steps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        buffer_sample = self.replay_buffer.sample(self.batch_size, beta=0.5)\n",
    "        obs = torch.from_numpy(buffer_sample[0]).float().to(DEVICE)\n",
    "        act = torch.from_numpy(buffer_sample[1]).long().to(DEVICE)\n",
    "        rew = torch.from_numpy(buffer_sample[2]).float().to(DEVICE)\n",
    "        obs1 = torch.from_numpy(buffer_sample[3]).float().to(DEVICE)        \n",
    "        dones = torch.from_numpy(buffer_sample[4].astype(int)).float().to(DEVICE)    \n",
    "        wt = torch.from_numpy(buffer_sample[5]).float().to(DEVICE)    \n",
    "        idxes = buffer_sample[6]\n",
    "        \n",
    "        self.dbqn.train()\n",
    "        \n",
    "        val1 = self.target_dbqn(obs1, use_sample=False).detach()\n",
    "        _, max_act = val1.max(1)\n",
    "        val1 = val1.gather(1, max_act.view(-1, 1)).squeeze()        \n",
    "        targets = rew + self.gamma * val1 * (1 - dones)     \n",
    "        \n",
    "        log_prior, log_variational_posterior, mse, td_errors = self.dbqn.sample_elbo(obs, act, targets)                \n",
    "        loss = (log_variational_posterior - log_prior) + mse        \n",
    "                \n",
    "        weighted_loss = (wt * loss).mean()        \n",
    "        self.optimizer.zero_grad()\n",
    "                \n",
    "        weighted_loss.backward()        \n",
    "        self.optimizer.step()\n",
    "                \n",
    "        for p in self.dbqn.parameters():\n",
    "            if torch.isnan(p).sum().item() == 1:\n",
    "                pdb.set_trace()\n",
    "\n",
    "        writer.add_scalar('data/loss', weighted_loss.detach().numpy(), self.t)        \n",
    "        writer.add_scalar('data/prior', log_prior.detach().cpu().numpy(), self.t)\n",
    "        writer.add_scalar('data/posterior', log_variational_posterior.detach().cpu().numpy(), self.t)\n",
    "        writer.add_scalar('data/mse', mse.mean().detach().cpu().numpy(), self.t)        \n",
    "                        \n",
    "        writer.add_scalar('data/w1_sigma', np.mean(self.dbqn.l1.weight.sigma[0].detach().cpu().numpy()).item(), self.t)              \n",
    "        writer.add_scalar('data/w2_sigma', np.mean(self.dbqn.l2.weight.sigma[0].detach().cpu().numpy()).item(), self.t)        \n",
    "        \n",
    "        \n",
    "        return idxes, td_errors               \n",
    "        \n",
    "    def step(self, obs_t, act_t, rew_t, obs_t1, done ):\n",
    "        \n",
    "        self.replay_buffer.add(obs_t, act_t, rew_t, obs_t1, done)\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        if self.t%self.train_freq == 0 and self.t > self.batch_size:\n",
    "            idxes, td_errors = self.train()\n",
    "            self.replay_buffer.update_priorities(idxes, np.abs(td_errors) + 1e-6)\n",
    "            \n",
    "        if self.t%self.update_freq == 0 and self.t > self.batch_size:\n",
    "            self.update_target()\n",
    "        \n",
    "        return self.act(obs_t1, use_sample=True, num_sample=0)\n",
    "        \n",
    "    def act(self, obs, use_sample, num_sample):\n",
    "        \n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0).to(DEVICE)   \n",
    "        return np.argmax(self.dbqn(obs, use_sample, num_sample).detach().cpu().numpy())\n",
    "        \n",
    "    def reset(self, obs):        \n",
    "        self.t = self.t + 1\n",
    "        return self.act(obs, use_sample=True, num_sample=2)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_dbqn.load_state_dict(self.dbqn.state_dict())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_agent(agent):\n",
    "    \n",
    "    count = 0    \n",
    "    test_episode_rew = 0\n",
    "    test_return = []\n",
    "    \n",
    "    done = False\n",
    "    agent.dbqn.eval()\n",
    "    \n",
    "    obs = env.reset()\n",
    "    act = agent.act(obs, use_sample=True, num_sample=4)\n",
    "    \n",
    "    while count <= 99:\n",
    "        if done:\n",
    "            test_return.append(test_episode_rew)\n",
    "#             print(test_episode_rew)\n",
    "            test_episode_rew = 0\n",
    "            count = count + 1\n",
    "            \n",
    "            obs = env.reset()\n",
    "            act = agent.act(obs, use_sample=True, num_sample=4)\n",
    "        \n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.act(obs1, use_sample=True, num_sample=0)        \n",
    "        test_episode_rew = test_episode_rew + rew        \n",
    "    \n",
    "    agent.dbqn.train()\n",
    "    return np.mean(np.array(test_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runs = 5\n",
    "run_result = []\n",
    "\n",
    "# lambda1 = lambda lr: lr * 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with reward = 10.0\n",
      "Episode 1 with reward = 9.0\n",
      "Episode 2 with reward = 10.0\n",
      "Episode 3 with reward = 14.0\n",
      "Episode 4 with reward = 11.0\n",
      "Episode 5 with reward = 10.0\n",
      "Episode 6 with reward = 10.0\n",
      "Episode 7 with reward = 12.0\n",
      "Episode 8 with reward = 11.0\n",
      "Episode 9 with reward = 30.0\n",
      "Episode 10 with reward = 10.0\n",
      "Episode 11 with reward = 11.0\n",
      "Episode 12 with reward = 14.0\n",
      "Episode 13 with reward = 11.0\n",
      "Episode 14 with reward = 9.0\n",
      "Episode 15 with reward = 10.0\n",
      "Episode 16 with reward = 11.0\n",
      "Episode 17 with reward = 15.0\n",
      "Episode 18 with reward = 10.0\n",
      "Episode 19 with reward = 10.0\n",
      "Episode 20 with reward = 10.0\n",
      "Episode 21 with reward = 9.0\n",
      "Episode 22 with reward = 9.0\n",
      "Episode 23 with reward = 11.0\n",
      "Episode 24 with reward = 10.0\n",
      "Test Result = 9.59\n",
      "Episode 25 with reward = 13.0\n",
      "Episode 26 with reward = 12.0\n",
      "Episode 27 with reward = 9.0\n",
      "Episode 28 with reward = 10.0\n",
      "Episode 29 with reward = 9.0\n",
      "Episode 30 with reward = 9.0\n",
      "Episode 31 with reward = 12.0\n",
      "Episode 32 with reward = 9.0\n",
      "Episode 33 with reward = 20.0\n",
      "Episode 34 with reward = 14.0\n",
      "Episode 35 with reward = 10.0\n",
      "Episode 36 with reward = 11.0\n",
      "Episode 37 with reward = 13.0\n",
      "Episode 38 with reward = 17.0\n",
      "Episode 39 with reward = 11.0\n",
      "Episode 40 with reward = 11.0\n",
      "Episode 41 with reward = 15.0\n",
      "Episode 42 with reward = 25.0\n",
      "Episode 43 with reward = 18.0\n",
      "Episode 44 with reward = 14.0\n",
      "Episode 45 with reward = 20.0\n",
      "Episode 46 with reward = 47.0\n",
      "Episode 47 with reward = 12.0\n",
      "Episode 48 with reward = 45.0\n",
      "Episode 49 with reward = 32.0\n",
      "Test Result = 32.54\n",
      "Episode 50 with reward = 16.0\n",
      "Episode 51 with reward = 22.0\n",
      "Episode 52 with reward = 18.0\n",
      "Episode 53 with reward = 26.0\n",
      "Episode 54 with reward = 29.0\n",
      "Episode 55 with reward = 21.0\n",
      "Episode 56 with reward = 33.0\n",
      "Episode 57 with reward = 14.0\n",
      "Episode 58 with reward = 24.0\n",
      "Episode 59 with reward = 30.0\n",
      "Episode 60 with reward = 19.0\n",
      "Episode 61 with reward = 14.0\n",
      "Episode 62 with reward = 13.0\n",
      "Episode 63 with reward = 22.0\n",
      "Episode 64 with reward = 26.0\n",
      "Episode 65 with reward = 16.0\n",
      "Episode 66 with reward = 11.0\n",
      "Episode 67 with reward = 32.0\n",
      "Episode 68 with reward = 20.0\n",
      "Episode 69 with reward = 20.0\n",
      "Episode 70 with reward = 20.0\n",
      "Episode 71 with reward = 22.0\n",
      "Episode 72 with reward = 36.0\n",
      "Episode 73 with reward = 26.0\n",
      "Episode 74 with reward = 12.0\n",
      "Test Result = 22.32\n",
      "Episode 75 with reward = 21.0\n",
      "Episode 76 with reward = 18.0\n",
      "Episode 77 with reward = 23.0\n",
      "Episode 78 with reward = 9.0\n",
      "Episode 79 with reward = 24.0\n",
      "Episode 80 with reward = 23.0\n",
      "Episode 81 with reward = 30.0\n",
      "Episode 82 with reward = 50.0\n",
      "Episode 83 with reward = 28.0\n",
      "Episode 84 with reward = 17.0\n",
      "Episode 85 with reward = 21.0\n",
      "Episode 86 with reward = 18.0\n",
      "Episode 87 with reward = 20.0\n",
      "Episode 88 with reward = 30.0\n",
      "Episode 89 with reward = 16.0\n",
      "Episode 90 with reward = 25.0\n",
      "Episode 91 with reward = 14.0\n",
      "Episode 92 with reward = 18.0\n",
      "Episode 93 with reward = 31.0\n",
      "Episode 94 with reward = 72.0\n",
      "Episode 95 with reward = 54.0\n",
      "Episode 96 with reward = 23.0\n",
      "Episode 97 with reward = 26.0\n",
      "Episode 98 with reward = 84.0\n",
      "Episode 99 with reward = 88.0\n",
      "Test Result = 24.77\n",
      "Episode 100 with reward = 56.0\n",
      "Episode 101 with reward = 57.0\n",
      "Episode 102 with reward = 45.0\n",
      "Episode 103 with reward = 21.0\n",
      "Episode 104 with reward = 25.0\n",
      "Episode 105 with reward = 29.0\n",
      "Episode 106 with reward = 22.0\n",
      "Episode 107 with reward = 72.0\n",
      "Episode 108 with reward = 24.0\n",
      "Episode 109 with reward = 45.0\n",
      "Episode 110 with reward = 39.0\n",
      "Episode 111 with reward = 24.0\n",
      "Episode 112 with reward = 24.0\n",
      "Episode 113 with reward = 40.0\n",
      "Episode 114 with reward = 38.0\n",
      "Episode 115 with reward = 28.0\n",
      "Episode 116 with reward = 62.0\n",
      "Episode 117 with reward = 34.0\n",
      "Episode 118 with reward = 55.0\n",
      "Episode 119 with reward = 14.0\n",
      "Episode 120 with reward = 50.0\n",
      "Episode 121 with reward = 18.0\n",
      "Episode 122 with reward = 34.0\n",
      "Episode 123 with reward = 28.0\n",
      "Episode 124 with reward = 63.0\n",
      "Test Result = 26.66\n",
      "Episode 125 with reward = 37.0\n",
      "Episode 126 with reward = 33.0\n",
      "Episode 127 with reward = 30.0\n",
      "Episode 128 with reward = 49.0\n",
      "Episode 129 with reward = 22.0\n",
      "Episode 130 with reward = 41.0\n",
      "Episode 131 with reward = 18.0\n",
      "Episode 132 with reward = 64.0\n",
      "Episode 133 with reward = 34.0\n",
      "Episode 134 with reward = 48.0\n",
      "Episode 135 with reward = 16.0\n",
      "Episode 136 with reward = 39.0\n",
      "Episode 137 with reward = 23.0\n",
      "Episode 138 with reward = 45.0\n",
      "Episode 139 with reward = 71.0\n",
      "Episode 140 with reward = 56.0\n",
      "Episode 141 with reward = 38.0\n",
      "Episode 142 with reward = 14.0\n",
      "Episode 143 with reward = 16.0\n",
      "Episode 144 with reward = 27.0\n",
      "Episode 145 with reward = 28.0\n",
      "Episode 146 with reward = 48.0\n",
      "Episode 147 with reward = 44.0\n",
      "Episode 148 with reward = 47.0\n",
      "Episode 149 with reward = 27.0\n",
      "Test Result = 31.83\n",
      "Episode 150 with reward = 17.0\n",
      "Episode 151 with reward = 32.0\n",
      "Episode 152 with reward = 17.0\n",
      "Episode 153 with reward = 28.0\n",
      "Episode 154 with reward = 112.0\n",
      "Episode 155 with reward = 67.0\n",
      "Episode 156 with reward = 48.0\n",
      "Episode 157 with reward = 61.0\n",
      "Episode 158 with reward = 88.0\n",
      "Episode 159 with reward = 59.0\n",
      "Episode 160 with reward = 60.0\n",
      "Episode 161 with reward = 39.0\n",
      "Episode 162 with reward = 18.0\n",
      "Episode 163 with reward = 22.0\n",
      "Episode 164 with reward = 16.0\n",
      "Episode 165 with reward = 40.0\n",
      "Episode 166 with reward = 34.0\n",
      "Episode 167 with reward = 65.0\n",
      "Episode 168 with reward = 57.0\n",
      "Episode 169 with reward = 20.0\n",
      "Episode 170 with reward = 88.0\n",
      "Episode 171 with reward = 45.0\n",
      "Episode 172 with reward = 51.0\n",
      "Episode 173 with reward = 21.0\n",
      "Episode 174 with reward = 17.0\n",
      "Test Result = 48.38\n",
      "Episode 175 with reward = 70.0\n",
      "Episode 176 with reward = 47.0\n",
      "Episode 177 with reward = 118.0\n",
      "Episode 178 with reward = 38.0\n",
      "Episode 179 with reward = 57.0\n",
      "Episode 180 with reward = 62.0\n",
      "Episode 181 with reward = 153.0\n",
      "Episode 182 with reward = 65.0\n",
      "Episode 183 with reward = 100.0\n",
      "Episode 184 with reward = 200.0\n",
      "Episode 185 with reward = 200.0\n",
      "Episode 186 with reward = 87.0\n",
      "Episode 187 with reward = 49.0\n",
      "Episode 188 with reward = 200.0\n",
      "Episode 189 with reward = 200.0\n",
      "Episode 190 with reward = 200.0\n",
      "Episode 191 with reward = 200.0\n",
      "Episode 192 with reward = 200.0\n",
      "Episode 193 with reward = 200.0\n",
      "Episode 194 with reward = 200.0\n",
      "Episode 195 with reward = 200.0\n",
      "Episode 196 with reward = 200.0\n",
      "Episode 197 with reward = 200.0\n",
      "Episode 198 with reward = 200.0\n",
      "Episode 199 with reward = 187.0\n",
      "Test Result = 172.63\n",
      "Episode 200 with reward = 142.0\n",
      "Episode 201 with reward = 200.0\n",
      "Episode 202 with reward = 200.0\n",
      "Episode 203 with reward = 200.0\n",
      "Episode 204 with reward = 193.0\n",
      "Episode 205 with reward = 200.0\n",
      "Episode 206 with reward = 200.0\n",
      "Episode 207 with reward = 200.0\n",
      "Episode 208 with reward = 197.0\n",
      "Episode 209 with reward = 183.0\n",
      "Episode 210 with reward = 200.0\n",
      "Episode 211 with reward = 200.0\n",
      "Episode 212 with reward = 200.0\n",
      "Episode 213 with reward = 200.0\n",
      "Episode 214 with reward = 200.0\n",
      "Episode 215 with reward = 200.0\n",
      "Episode 216 with reward = 200.0\n",
      "Episode 217 with reward = 200.0\n",
      "Episode 218 with reward = 200.0\n",
      "Episode 219 with reward = 200.0\n",
      "Episode 220 with reward = 200.0\n",
      "Episode 221 with reward = 200.0\n",
      "Episode 222 with reward = 200.0\n",
      "Episode 223 with reward = 200.0\n",
      "Episode 224 with reward = 200.0\n",
      "Test Result = 182.24\n",
      "Episode 225 with reward = 197.0\n",
      "Episode 226 with reward = 200.0\n",
      "Episode 227 with reward = 200.0\n",
      "Episode 228 with reward = 200.0\n",
      "Episode 229 with reward = 200.0\n",
      "Episode 230 with reward = 200.0\n",
      "Episode 231 with reward = 189.0\n",
      "Episode 232 with reward = 200.0\n",
      "Episode 233 with reward = 194.0\n",
      "Episode 234 with reward = 200.0\n",
      "Episode 235 with reward = 52.0\n",
      "Episode 236 with reward = 200.0\n",
      "Episode 237 with reward = 200.0\n",
      "Episode 238 with reward = 200.0\n",
      "Episode 239 with reward = 200.0\n",
      "Episode 240 with reward = 200.0\n",
      "Episode 241 with reward = 200.0\n",
      "Episode 242 with reward = 200.0\n",
      "Episode 243 with reward = 200.0\n",
      "Episode 244 with reward = 200.0\n",
      "Episode 245 with reward = 200.0\n",
      "Episode 246 with reward = 200.0\n",
      "Episode 247 with reward = 200.0\n",
      "Episode 248 with reward = 200.0\n",
      "Episode 249 with reward = 200.0\n",
      "Test Result = 158.55\n",
      "Episode 250 with reward = 200.0\n",
      "Episode 251 with reward = 200.0\n",
      "Episode 252 with reward = 200.0\n",
      "Episode 253 with reward = 200.0\n",
      "Episode 254 with reward = 200.0\n",
      "Episode 255 with reward = 200.0\n",
      "Episode 256 with reward = 200.0\n",
      "Episode 257 with reward = 117.0\n",
      "Episode 258 with reward = 200.0\n",
      "Episode 259 with reward = 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260 with reward = 200.0\n",
      "Episode 261 with reward = 200.0\n",
      "Episode 262 with reward = 200.0\n",
      "Episode 263 with reward = 200.0\n",
      "Episode 264 with reward = 200.0\n",
      "Episode 265 with reward = 200.0\n",
      "Episode 266 with reward = 200.0\n",
      "Episode 267 with reward = 200.0\n",
      "Episode 268 with reward = 200.0\n",
      "Episode 269 with reward = 200.0\n",
      "Episode 270 with reward = 200.0\n",
      "Episode 271 with reward = 200.0\n",
      "Episode 272 with reward = 121.0\n",
      "Episode 273 with reward = 177.0\n",
      "Episode 274 with reward = 200.0\n",
      "Test Result = 171.21\n",
      "Episode 275 with reward = 200.0\n",
      "Episode 276 with reward = 200.0\n",
      "Episode 277 with reward = 200.0\n",
      "Episode 278 with reward = 200.0\n",
      "Episode 279 with reward = 200.0\n",
      "Episode 280 with reward = 200.0\n",
      "Episode 281 with reward = 200.0\n",
      "Episode 282 with reward = 200.0\n",
      "Episode 283 with reward = 200.0\n",
      "Episode 284 with reward = 200.0\n",
      "Episode 285 with reward = 200.0\n",
      "Episode 286 with reward = 164.0\n",
      "Episode 287 with reward = 200.0\n",
      "Episode 288 with reward = 200.0\n"
     ]
    }
   ],
   "source": [
    "for run in range(runs):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    batch_size = 32\n",
    "    gamma = 0.95\n",
    "    STEPS = 30000\n",
    "    writer = SummaryWriter()\n",
    "    dbqn = BayesianNetwork(env, 4, batch_size).to(DEVICE)    \n",
    "    target_dbqn = BayesianNetwork(env, 4, batch_size).to(DEVICE)\n",
    "    agent = DBQN_learn(dbqn, target_dbqn, gamma, lr, batch_size, STEPS)\n",
    "    \n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(agent.optimizer, lr_lambda=lambda1)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    episode_rew = 0\n",
    "    episode_count = 0\n",
    "    res = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    act = agent.reset(obs)         \n",
    "\n",
    "    while agent.t <= STEPS or episode_count < 300:\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode \" + str(episode_count) + \" with reward = \" + str(episode_rew))  \n",
    "            writer.add_scalar('data/reward', episode_rew, episode_count)\n",
    "            res.append(episode_rew)\n",
    "            episode_rew = 0\n",
    "            episode_count = episode_count + 1                \n",
    "            \n",
    "#             if episode_count < 100:\n",
    "#                 scheduler.step()\n",
    "\n",
    "            if episode_count%25 == 0:\n",
    "                test_result = test_agent(agent)\n",
    "                print(\"Test Result = \" + str(test_result))\n",
    "                writer.add_scalar('data/test_reward', test_result, episode_count)\n",
    "\n",
    "            obs = env.reset()\n",
    "            act = agent.reset(obs)   \n",
    "\n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.step(obs, act, rew, obs1, done)\n",
    "        obs = obs1\n",
    "        episode_rew = episode_rew + rew    \n",
    "            \n",
    "    writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "    writer.close()\n",
    "        \n",
    "    run_result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_length = 10000\n",
    "for i in range(len(run_result)):    \n",
    "    if min_length > len(run_result[i]):\n",
    "        min_length = len(run_result[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_result = [run_result[i][:min_length] for i in range(10)]\n",
    "tmp_result = np.stack( tmp_result, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.mean(tmp_result, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
