{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BayesianLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        #Weight Parameters \n",
    "        self.init_weight_mean = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.init_weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-2, -1))\n",
    "        self.weight = DifferentiableGaussian(self.init_weight_mean, self.init_weight_rho)\n",
    "        \n",
    "        #Bias Parameters \n",
    "        self.init_bias_mean = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.init_bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-2, -1))\n",
    "        self.bias = DifferentiableGaussian(self.init_bias_mean, self.init_bias_rho)\n",
    "        \n",
    "        #Prior Distributions\n",
    "        pi = 0.5\n",
    "        prior_sigma1 = torch.FloatTensor([math.exp(-0)])\n",
    "        prior_sigma2 = torch.FloatTensor([math.exp(1)])\n",
    "        self.weight_prior = ScaleMixtureGaussian(pi, prior_sigma1, prior_sigma2)\n",
    "        self.bias_prior = ScaleMixtureGaussian(pi, prior_sigma1, prior_sigma2)\n",
    "        \n",
    "        self.weight_sample, self.bias_sample = self.weight.sample().detach(), self.bias.sample().detach()\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "        \n",
    "    def sample(self, MC_samples=4):\n",
    "        wt_val = []; bias_val = []\n",
    "        for i in range(MC_samples):\n",
    "            wt_val.append(self.weight.sample())\n",
    "            bias_val.append(self.bias.sample())\n",
    "            \n",
    "        return torch.mean(torch.stack(wt_val), dim=0).data, torch.mean(torch.stack(bias_val), dim=0).data    \n",
    "    \n",
    "    def forward(self, input, use_sample, num_sample=0):\n",
    "        \n",
    "        if torch.isnan(self.weight.mean).sum().item() == 1:\n",
    "            pdb.set_trace()\n",
    "            \n",
    "        if use_sample:       \n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()            \n",
    "            if num_sample:\n",
    "                self.weight_sample.data, self.bias_sample.data = self.sample()\n",
    "            weight.data = self.weight_sample\n",
    "            bias.data = self.bias_sample\n",
    "                            \n",
    "        else:\n",
    "            weight = self.weight.mean\n",
    "            bias = self.bias.mean\n",
    "        if self.training :\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            self.log_prior, self.log_variational_posterior = 0, 0\n",
    "        \n",
    "        return F.linear(input, weight, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BayesianNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_features, output_features, num_samples, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        self.l1 = BayesianLinear(env.observation_space.shape[0], 32)\n",
    "        self.l2 = BayesianLinear(32, env.action_space.n)\n",
    "        \n",
    "        layer_arr = [self.l1, self.l2] \n",
    "        self.layer_arr = nn.ModuleList(layer_arr)       \n",
    "        self.layer_num = len(layer_arr)\n",
    "                \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, x, use_sample, num_sample=0):        \n",
    "        for i in range(self.layer_num - 1):            \n",
    "            x = F.relu(self.layer_arr[i](x, use_sample, num_sample))            \n",
    "        x = self.layer_arr[i+1](x, use_sample, num_sample)\n",
    "        return x\n",
    "        \n",
    "    def log_prior(self):\n",
    "        log_prior = 0        \n",
    "        for i in range(self.layer_num):\n",
    "            log_prior = log_prior + self.layer_arr[i].log_prior\n",
    "        return log_prior\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        log_posterior = 0\n",
    "#         pdb.set_trace()\n",
    "        for i in range(self.layer_num):\n",
    "            log_posterior = log_posterior + self.layer_arr[i].log_variational_posterior\n",
    "        return log_posterior\n",
    "            \n",
    "    def sample_elbo(self, obs, act, targets):\n",
    "        \n",
    "        outputs = torch.zeros(self.num_samples, self.batch_size).to(DEVICE)\n",
    "        log_prior = torch.zeros(self.num_samples).to(DEVICE)\n",
    "        log_variational_posterior = torch.zeros(self.num_samples).to(DEVICE)\n",
    "        for i in range(self.num_samples):\n",
    "            val = self.forward(obs, use_sample=True, num_sample=1)\n",
    "            outputs[i] = val.gather(1, act.view(-1, 1)).squeeze()\n",
    "            log_prior[i] = log_prior[i] + self.log_prior()/STEPS\n",
    "            log_variational_posterior[i] = log_variational_posterior[i] + self.log_variational_posterior()/STEPS\n",
    "        \n",
    "        loss = nn.MSELoss(reduction='none')\n",
    "        mse = loss(outputs.mean(0), targets)\n",
    "        log_prior = log_prior.mean()\n",
    "#         log_prior.data = torch.clamp(log_prior.data, -1000, 1000)        \n",
    "        log_variational_posterior = log_variational_posterior.mean()\n",
    "        td_errors = outputs.mean(0) - targets\n",
    "        \n",
    "        return log_prior, log_variational_posterior, mse, td_errors.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DBQN_learn():\n",
    "    \n",
    "    def __init__(self, model, target_model, gamma, lr, batch_size, num_steps):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dbqn = model\n",
    "        self.target_dbqn = target_model\n",
    "        self.update_target()\n",
    "        \n",
    "        self.t = 0\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(lr = lr, params = self.dbqn.parameters())\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "        self.train_freq = 1\n",
    "        self.update_freq = 100\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(10000, 0.7)\n",
    "        self.constt = 1/self.num_steps\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        buffer_sample = self.replay_buffer.sample(self.batch_size, beta=0.5)\n",
    "        obs = torch.from_numpy(buffer_sample[0]).float().to(DEVICE)\n",
    "        act = torch.from_numpy(buffer_sample[1]).long().to(DEVICE)\n",
    "        rew = torch.from_numpy(buffer_sample[2]).float().to(DEVICE)\n",
    "        obs1 = torch.from_numpy(buffer_sample[3]).float().to(DEVICE)        \n",
    "        dones = torch.from_numpy(buffer_sample[4].astype(int)).float().to(DEVICE)    \n",
    "        wt = torch.from_numpy(buffer_sample[5]).float().to(DEVICE)    \n",
    "        idxes = buffer_sample[6]\n",
    "        \n",
    "        self.dbqn.train()\n",
    "        \n",
    "        val1 = self.target_dbqn(obs1, use_sample=False).detach()\n",
    "        _, max_act = val1.max(1)\n",
    "        val1 = val1.gather(1, max_act.view(-1, 1)).squeeze()        \n",
    "        targets = rew + self.gamma * val1 * (1 - dones)     \n",
    "        \n",
    "        log_prior, log_variational_posterior, mse, td_errors = self.dbqn.sample_elbo(obs, act, targets)                \n",
    "        loss = (log_variational_posterior - log_prior) + mse        \n",
    "                \n",
    "        weighted_loss = (wt * loss).mean()        \n",
    "        self.optimizer.zero_grad()\n",
    "                \n",
    "        weighted_loss.backward()\n",
    "        old_wt = []\n",
    "        for p in self.dbqn.parameters():\n",
    "            old_wt.append(deepcopy(p))            \n",
    "        self.optimizer.step()\n",
    "                \n",
    "        for p in self.dbqn.parameters():\n",
    "            if torch.isnan(p).sum().item() == 1:\n",
    "                pdb.set_trace()\n",
    "#         writer.add_scalar('data/constt', self.constt, self.t)\n",
    "        writer.add_scalar('data/loss', weighted_loss.detach().numpy(), self.t)        \n",
    "        writer.add_scalar('data/prior', log_prior.detach().cpu().numpy(), self.t)\n",
    "        writer.add_scalar('data/posterior', log_variational_posterior.detach().cpu().numpy(), self.t)\n",
    "        writer.add_scalar('data/mse', mse.mean().detach().cpu().numpy(), self.t)        \n",
    "                        \n",
    "#         writer.add_scalar('data/w1_mu', self.dbqn.l1.weight.mean[0][TMP1].detach().cpu().numpy().item(), self.t)\n",
    "        writer.add_scalar('data/w1_sigma', np.mean(self.dbqn.l1.weight.sigma[0].detach().cpu().numpy()).item(), self.t)\n",
    "              \n",
    "#         writer.add_scalar('data/w2_mu', self.dbqn.l2.weight.mean[0][TMP2].detach().cpu().numpy().item(), self.t)\n",
    "        writer.add_scalar('data/w2_sigma', np.mean(self.dbqn.l2.weight.sigma[0].detach().cpu().numpy()).item(), self.t)        \n",
    "        \n",
    "        \n",
    "        return idxes, td_errors               \n",
    "        \n",
    "    def step(self, obs_t, act_t, rew_t, obs_t1, done ):\n",
    "        \n",
    "        self.replay_buffer.add(obs_t, act_t, rew_t, obs_t1, done)\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        if self.t%self.train_freq == 0 and self.t > self.batch_size:\n",
    "            idxes, td_errors = self.train()\n",
    "            self.replay_buffer.update_priorities(idxes, np.abs(td_errors) + 1e-6)\n",
    "            \n",
    "        if self.t%self.update_freq == 0 and self.t > self.batch_size:\n",
    "            self.update_target()\n",
    "        \n",
    "        return self.act(obs_t1, use_sample=True, num_sample=0)\n",
    "        \n",
    "    def act(self, obs, use_sample, num_sample):\n",
    "        \n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0).to(DEVICE)   \n",
    "        return np.argmax(self.dbqn(obs, use_sample, num_sample).detach().cpu().numpy())\n",
    "        \n",
    "    def reset(self, obs):        \n",
    "        self.t = self.t + 1\n",
    "        return self.act(obs, use_sample=True, num_sample=2)\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_dbqn.load_state_dict(self.dbqn.state_dict())        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_agent(agent):\n",
    "    \n",
    "    count = 0    \n",
    "    test_episode_rew = 0\n",
    "    test_return = []\n",
    "    \n",
    "    done = False\n",
    "    agent.dbqn.eval()\n",
    "    \n",
    "    obs = env.reset()\n",
    "    act = agent.act(obs, use_sample=True, num_sample=4)\n",
    "    \n",
    "    while count <= 99:\n",
    "        if done:\n",
    "            test_return.append(test_episode_rew)\n",
    "#             print(test_episode_rew)\n",
    "            test_episode_rew = 0\n",
    "            count = count + 1\n",
    "            \n",
    "            obs = env.reset()\n",
    "            act = agent.act(obs, use_sample=True, num_sample=4)\n",
    "        \n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.act(obs1, use_sample=True, num_sample=0)        \n",
    "        test_episode_rew = test_episode_rew + rew        \n",
    "    \n",
    "    agent.dbqn.train()\n",
    "    return np.mean(np.array(test_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runs = 5\n",
    "run_result = []\n",
    "\n",
    "# lambda1 = lambda lr: lr * 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with reward = 10.0\n",
      "Episode 1 with reward = 9.0\n",
      "Episode 2 with reward = 10.0\n",
      "Episode 3 with reward = 14.0\n",
      "Episode 4 with reward = 11.0\n",
      "Episode 5 with reward = 10.0\n",
      "Episode 6 with reward = 10.0\n",
      "Episode 7 with reward = 12.0\n",
      "Episode 8 with reward = 11.0\n",
      "Episode 9 with reward = 30.0\n",
      "Episode 10 with reward = 10.0\n",
      "Episode 11 with reward = 11.0\n",
      "Episode 12 with reward = 14.0\n",
      "Episode 13 with reward = 11.0\n",
      "Episode 14 with reward = 9.0\n",
      "Episode 15 with reward = 10.0\n",
      "Episode 16 with reward = 11.0\n",
      "Episode 17 with reward = 15.0\n",
      "Episode 18 with reward = 10.0\n",
      "Episode 19 with reward = 10.0\n",
      "Episode 20 with reward = 10.0\n",
      "Episode 21 with reward = 9.0\n",
      "Episode 22 with reward = 9.0\n",
      "Episode 23 with reward = 11.0\n",
      "Episode 24 with reward = 10.0\n",
      "Test Result = 9.59\n",
      "Episode 25 with reward = 13.0\n",
      "Episode 26 with reward = 12.0\n",
      "Episode 27 with reward = 9.0\n",
      "Episode 28 with reward = 10.0\n",
      "Episode 29 with reward = 9.0\n",
      "Episode 30 with reward = 9.0\n",
      "Episode 31 with reward = 12.0\n",
      "Episode 32 with reward = 9.0\n",
      "Episode 33 with reward = 20.0\n",
      "Episode 34 with reward = 14.0\n",
      "Episode 35 with reward = 10.0\n",
      "Episode 36 with reward = 11.0\n",
      "Episode 37 with reward = 13.0\n",
      "Episode 38 with reward = 17.0\n",
      "Episode 39 with reward = 11.0\n",
      "Episode 40 with reward = 11.0\n",
      "Episode 41 with reward = 15.0\n",
      "Episode 42 with reward = 25.0\n",
      "Episode 43 with reward = 18.0\n",
      "Episode 44 with reward = 14.0\n",
      "Episode 45 with reward = 20.0\n",
      "Episode 46 with reward = 47.0\n",
      "Episode 47 with reward = 12.0\n",
      "Episode 48 with reward = 45.0\n",
      "Episode 49 with reward = 32.0\n",
      "Test Result = 32.54\n",
      "Episode 50 with reward = 16.0\n",
      "Episode 51 with reward = 22.0\n",
      "Episode 52 with reward = 18.0\n",
      "Episode 53 with reward = 26.0\n",
      "Episode 54 with reward = 29.0\n",
      "Episode 55 with reward = 21.0\n",
      "Episode 56 with reward = 33.0\n",
      "Episode 57 with reward = 14.0\n",
      "Episode 58 with reward = 24.0\n",
      "Episode 59 with reward = 30.0\n",
      "Episode 60 with reward = 19.0\n",
      "Episode 61 with reward = 14.0\n",
      "Episode 62 with reward = 13.0\n",
      "Episode 63 with reward = 22.0\n",
      "Episode 64 with reward = 26.0\n",
      "Episode 65 with reward = 16.0\n",
      "Episode 66 with reward = 11.0\n",
      "Episode 67 with reward = 32.0\n",
      "Episode 68 with reward = 20.0\n",
      "Episode 69 with reward = 20.0\n",
      "Episode 70 with reward = 20.0\n",
      "Episode 71 with reward = 22.0\n",
      "Episode 72 with reward = 36.0\n",
      "Episode 73 with reward = 26.0\n",
      "Episode 74 with reward = 12.0\n",
      "Test Result = 22.32\n",
      "Episode 75 with reward = 21.0\n",
      "Episode 76 with reward = 18.0\n",
      "Episode 77 with reward = 23.0\n",
      "Episode 78 with reward = 9.0\n",
      "Episode 79 with reward = 24.0\n",
      "Episode 80 with reward = 23.0\n",
      "Episode 81 with reward = 30.0\n",
      "Episode 82 with reward = 50.0\n",
      "Episode 83 with reward = 28.0\n",
      "Episode 84 with reward = 17.0\n",
      "Episode 85 with reward = 21.0\n",
      "Episode 86 with reward = 18.0\n",
      "Episode 87 with reward = 20.0\n",
      "Episode 88 with reward = 30.0\n",
      "Episode 89 with reward = 16.0\n",
      "Episode 90 with reward = 25.0\n",
      "Episode 91 with reward = 14.0\n",
      "Episode 92 with reward = 18.0\n",
      "Episode 93 with reward = 31.0\n",
      "Episode 94 with reward = 72.0\n",
      "Episode 95 with reward = 54.0\n",
      "Episode 96 with reward = 23.0\n",
      "Episode 97 with reward = 26.0\n",
      "Episode 98 with reward = 84.0\n",
      "Episode 99 with reward = 88.0\n",
      "Test Result = 24.77\n",
      "Episode 100 with reward = 56.0\n",
      "Episode 101 with reward = 57.0\n",
      "Episode 102 with reward = 45.0\n",
      "Episode 103 with reward = 21.0\n",
      "Episode 104 with reward = 25.0\n",
      "Episode 105 with reward = 29.0\n",
      "Episode 106 with reward = 22.0\n",
      "Episode 107 with reward = 72.0\n",
      "Episode 108 with reward = 24.0\n",
      "Episode 109 with reward = 45.0\n",
      "Episode 110 with reward = 39.0\n",
      "Episode 111 with reward = 24.0\n",
      "Episode 112 with reward = 24.0\n",
      "Episode 113 with reward = 40.0\n",
      "Episode 114 with reward = 38.0\n",
      "Episode 115 with reward = 28.0\n",
      "Episode 116 with reward = 62.0\n",
      "Episode 117 with reward = 34.0\n",
      "Episode 118 with reward = 55.0\n",
      "Episode 119 with reward = 14.0\n",
      "Episode 120 with reward = 50.0\n",
      "Episode 121 with reward = 18.0\n",
      "Episode 122 with reward = 34.0\n",
      "Episode 123 with reward = 28.0\n",
      "Episode 124 with reward = 63.0\n",
      "Test Result = 26.66\n",
      "Episode 125 with reward = 37.0\n",
      "Episode 126 with reward = 33.0\n",
      "Episode 127 with reward = 30.0\n",
      "Episode 128 with reward = 49.0\n",
      "Episode 129 with reward = 22.0\n",
      "Episode 130 with reward = 41.0\n",
      "Episode 131 with reward = 18.0\n",
      "Episode 132 with reward = 64.0\n",
      "Episode 133 with reward = 34.0\n",
      "Episode 134 with reward = 48.0\n",
      "Episode 135 with reward = 16.0\n",
      "Episode 136 with reward = 39.0\n",
      "Episode 137 with reward = 23.0\n",
      "Episode 138 with reward = 45.0\n",
      "Episode 139 with reward = 71.0\n",
      "Episode 140 with reward = 56.0\n",
      "Episode 141 with reward = 38.0\n",
      "Episode 142 with reward = 14.0\n",
      "Episode 143 with reward = 16.0\n",
      "Episode 144 with reward = 27.0\n",
      "Episode 145 with reward = 28.0\n",
      "Episode 146 with reward = 48.0\n",
      "Episode 147 with reward = 44.0\n",
      "Episode 148 with reward = 47.0\n",
      "Episode 149 with reward = 27.0\n",
      "Test Result = 31.83\n",
      "Episode 150 with reward = 17.0\n",
      "Episode 151 with reward = 32.0\n",
      "Episode 152 with reward = 17.0\n",
      "Episode 153 with reward = 28.0\n",
      "Episode 154 with reward = 112.0\n",
      "Episode 155 with reward = 67.0\n",
      "Episode 156 with reward = 48.0\n",
      "Episode 157 with reward = 61.0\n",
      "Episode 158 with reward = 88.0\n",
      "Episode 159 with reward = 59.0\n",
      "Episode 160 with reward = 60.0\n",
      "Episode 161 with reward = 39.0\n",
      "Episode 162 with reward = 18.0\n",
      "Episode 163 with reward = 22.0\n",
      "Episode 164 with reward = 16.0\n",
      "Episode 165 with reward = 40.0\n",
      "Episode 166 with reward = 34.0\n",
      "Episode 167 with reward = 65.0\n",
      "Episode 168 with reward = 57.0\n",
      "Episode 169 with reward = 20.0\n",
      "Episode 170 with reward = 88.0\n",
      "Episode 171 with reward = 45.0\n",
      "Episode 172 with reward = 51.0\n",
      "Episode 173 with reward = 21.0\n",
      "Episode 174 with reward = 17.0\n",
      "Test Result = 48.38\n",
      "Episode 175 with reward = 70.0\n",
      "Episode 176 with reward = 47.0\n",
      "Episode 177 with reward = 118.0\n",
      "Episode 178 with reward = 38.0\n",
      "Episode 179 with reward = 57.0\n",
      "Episode 180 with reward = 62.0\n",
      "Episode 181 with reward = 153.0\n",
      "Episode 182 with reward = 65.0\n",
      "Episode 183 with reward = 100.0\n",
      "Episode 184 with reward = 200.0\n",
      "Episode 185 with reward = 200.0\n",
      "Episode 186 with reward = 87.0\n",
      "Episode 187 with reward = 49.0\n",
      "Episode 188 with reward = 200.0\n",
      "Episode 189 with reward = 200.0\n",
      "Episode 190 with reward = 200.0\n",
      "Episode 191 with reward = 200.0\n",
      "Episode 192 with reward = 200.0\n",
      "Episode 193 with reward = 200.0\n",
      "Episode 194 with reward = 200.0\n",
      "Episode 195 with reward = 200.0\n",
      "Episode 196 with reward = 200.0\n",
      "Episode 197 with reward = 200.0\n",
      "Episode 198 with reward = 200.0\n",
      "Episode 199 with reward = 187.0\n",
      "Test Result = 172.63\n",
      "Episode 200 with reward = 142.0\n",
      "Episode 201 with reward = 200.0\n",
      "Episode 202 with reward = 200.0\n",
      "Episode 203 with reward = 200.0\n",
      "Episode 204 with reward = 193.0\n",
      "Episode 205 with reward = 200.0\n",
      "Episode 206 with reward = 200.0\n",
      "Episode 207 with reward = 200.0\n",
      "Episode 208 with reward = 197.0\n",
      "Episode 209 with reward = 183.0\n",
      "Episode 210 with reward = 200.0\n",
      "Episode 211 with reward = 200.0\n",
      "Episode 212 with reward = 200.0\n",
      "Episode 213 with reward = 200.0\n",
      "Episode 214 with reward = 200.0\n",
      "Episode 215 with reward = 200.0\n",
      "Episode 216 with reward = 200.0\n",
      "Episode 217 with reward = 200.0\n",
      "Episode 218 with reward = 200.0\n",
      "Episode 219 with reward = 200.0\n",
      "Episode 220 with reward = 200.0\n",
      "Episode 221 with reward = 200.0\n",
      "Episode 222 with reward = 200.0\n",
      "Episode 223 with reward = 200.0\n",
      "Episode 224 with reward = 200.0\n",
      "Test Result = 182.24\n",
      "Episode 225 with reward = 197.0\n",
      "Episode 226 with reward = 200.0\n",
      "Episode 227 with reward = 200.0\n",
      "Episode 228 with reward = 200.0\n",
      "Episode 229 with reward = 200.0\n",
      "Episode 230 with reward = 200.0\n",
      "Episode 231 with reward = 189.0\n",
      "Episode 232 with reward = 200.0\n",
      "Episode 233 with reward = 194.0\n",
      "Episode 234 with reward = 200.0\n",
      "Episode 235 with reward = 52.0\n"
     ]
    }
   ],
   "source": [
    "for run in range(runs):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    batch_size = 32\n",
    "    gamma = 0.95\n",
    "    STEPS = 30000\n",
    "    writer = SummaryWriter()\n",
    "    dbqn = BayesianNetwork(env, 4, batch_size).to(DEVICE)    \n",
    "    target_dbqn = BayesianNetwork(env, 4, batch_size).to(DEVICE)\n",
    "    agent = DBQN_learn(dbqn, target_dbqn, gamma, lr, batch_size, STEPS)\n",
    "    \n",
    "#     scheduler = optim.lr_scheduler.LambdaLR(agent.optimizer, lr_lambda=lambda1)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    episode_rew = 0\n",
    "    episode_count = 0\n",
    "    res = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    act = agent.reset(obs)         \n",
    "\n",
    "    while agent.t <= STEPS or episode_count < 300:\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode \" + str(episode_count) + \" with reward = \" + str(episode_rew))  \n",
    "            writer.add_scalar('data/reward', episode_rew, episode_count)\n",
    "            res.append(episode_rew)\n",
    "            episode_rew = 0\n",
    "            episode_count = episode_count + 1                \n",
    "            \n",
    "#             if episode_count < 100:\n",
    "#                 scheduler.step()\n",
    "\n",
    "            if episode_count%25 == 0:\n",
    "                test_result = test_agent(agent)\n",
    "                print(\"Test Result = \" + str(test_result))\n",
    "                writer.add_scalar('data/test_reward', test_result, episode_count)\n",
    "\n",
    "            obs = env.reset()\n",
    "            act = agent.reset(obs)   \n",
    "\n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.step(obs, act, rew, obs1, done)\n",
    "        obs = obs1\n",
    "        episode_rew = episode_rew + rew    \n",
    "            \n",
    "    writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "    writer.close()\n",
    "        \n",
    "    run_result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_length = 10000\n",
    "for i in range(len(run_result)):    \n",
    "    if min_length > len(run_result[i]):\n",
    "        min_length = len(run_result[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_result = [run_result[i][:min_length] for i in range(10)]\n",
    "tmp_result = np.stack( tmp_result, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.mean(tmp_result, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
