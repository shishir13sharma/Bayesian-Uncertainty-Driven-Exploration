{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from BayesianNetwork import BayesianNetwork\n",
    "from BayesianQNetwork import BQN_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "envt = \"CartPole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_agent(agent):\n",
    "    \n",
    "    count = 0    \n",
    "    test_episode_rew = 0\n",
    "    test_return = []\n",
    "    \n",
    "    done = False\n",
    "    agent.dbqn.eval()\n",
    "    \n",
    "    obs = env.reset()\n",
    "    act = agent.act(obs, use_sample=False, num_sample=0)\n",
    "    \n",
    "    while count <= 99:\n",
    "        if done:\n",
    "            test_return.append(test_episode_rew)\n",
    "            test_episode_rew = 0\n",
    "            count = count + 1\n",
    "            \n",
    "            obs = env.reset()\n",
    "            act = agent.act(obs, use_sample=False, num_sample=0)\n",
    "        \n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.act(obs1, use_sample=False, num_sample=0)        \n",
    "        test_episode_rew = test_episode_rew + rew        \n",
    "    \n",
    "    agent.dbqn.train()\n",
    "    return np.mean(np.array(test_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "batch_size = 64\n",
    "gamma = 0.9\n",
    "if envt == \"CartPole\":   \n",
    "    lr = 1e-3\n",
    "    gamma = 0.9\n",
    "    steps = 20000    \n",
    "    buffer_size = 20000\n",
    "    features_list = [4, 32, 2]\n",
    "    env = gym.make('CartPole-v0')\n",
    "elif envt == \"Pendulum\":\n",
    "    steps = 200000\n",
    "    buffer_size = 50000\n",
    "    features_list = [2, 32, 5]\n",
    "    env = gym.make('Pendulum-v0')\n",
    "elif envt == \"MountainCar\":\n",
    "    steps = 200000\n",
    "    buffer_size = 50000\n",
    "    features_list = [2, 32, 3]\n",
    "    env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with reward = 8.0\n",
      "Episode 1 with reward = 41.0\n",
      "Episode 2 with reward = 13.0\n",
      "Episode 3 with reward = 37.0\n",
      "Episode 4 with reward = 31.0\n",
      "Episode 5 with reward = 17.0\n",
      "Episode 6 with reward = 13.0\n",
      "Episode 7 with reward = 12.0\n",
      "Episode 8 with reward = 20.0\n",
      "Episode 9 with reward = 14.0\n",
      "Episode 10 with reward = 44.0\n",
      "Episode 11 with reward = 13.0\n",
      "Episode 12 with reward = 24.0\n",
      "Episode 13 with reward = 16.0\n",
      "Episode 14 with reward = 11.0\n",
      "Episode 15 with reward = 25.0\n",
      "Episode 16 with reward = 29.0\n",
      "Episode 17 with reward = 14.0\n",
      "Episode 18 with reward = 26.0\n",
      "Episode 19 with reward = 32.0\n",
      "Episode 20 with reward = 95.0\n",
      "Episode 21 with reward = 33.0\n",
      "Episode 22 with reward = 13.0\n",
      "Episode 23 with reward = 11.0\n",
      "Episode 24 with reward = 29.0\n",
      "Test Result = 9.36\n",
      "Episode 25 with reward = 10.0\n",
      "Episode 26 with reward = 24.0\n",
      "Episode 27 with reward = 16.0\n",
      "Episode 28 with reward = 23.0\n",
      "Episode 29 with reward = 10.0\n",
      "Episode 30 with reward = 17.0\n",
      "Episode 31 with reward = 14.0\n",
      "Episode 32 with reward = 18.0\n",
      "Episode 33 with reward = 17.0\n",
      "Episode 34 with reward = 12.0\n",
      "Episode 35 with reward = 13.0\n",
      "Episode 36 with reward = 18.0\n",
      "Episode 37 with reward = 37.0\n",
      "Episode 38 with reward = 15.0\n",
      "Episode 39 with reward = 24.0\n",
      "Episode 40 with reward = 13.0\n",
      "Episode 41 with reward = 17.0\n",
      "Episode 42 with reward = 17.0\n",
      "Episode 43 with reward = 30.0\n",
      "Episode 44 with reward = 13.0\n",
      "Episode 45 with reward = 18.0\n",
      "Episode 46 with reward = 12.0\n",
      "Episode 47 with reward = 20.0\n",
      "Episode 48 with reward = 29.0\n",
      "Episode 49 with reward = 33.0\n",
      "Test Result = 9.28\n",
      "Episode 50 with reward = 17.0\n",
      "Episode 51 with reward = 14.0\n",
      "Episode 52 with reward = 14.0\n",
      "Episode 53 with reward = 9.0\n",
      "Episode 54 with reward = 15.0\n",
      "Episode 55 with reward = 12.0\n",
      "Episode 56 with reward = 15.0\n",
      "Episode 57 with reward = 21.0\n",
      "Episode 58 with reward = 18.0\n",
      "Episode 59 with reward = 18.0\n",
      "Episode 60 with reward = 36.0\n",
      "Episode 61 with reward = 43.0\n",
      "Episode 62 with reward = 26.0\n",
      "Episode 63 with reward = 15.0\n",
      "Episode 64 with reward = 32.0\n",
      "Episode 65 with reward = 11.0\n",
      "Episode 66 with reward = 22.0\n",
      "Episode 67 with reward = 13.0\n",
      "Episode 68 with reward = 9.0\n",
      "Episode 69 with reward = 14.0\n",
      "Episode 70 with reward = 21.0\n",
      "Episode 71 with reward = 39.0\n",
      "Episode 72 with reward = 30.0\n",
      "Episode 73 with reward = 21.0\n",
      "Episode 74 with reward = 16.0\n",
      "Test Result = 9.36\n",
      "Episode 75 with reward = 44.0\n",
      "Episode 76 with reward = 17.0\n",
      "Episode 77 with reward = 20.0\n",
      "Episode 78 with reward = 13.0\n",
      "Episode 79 with reward = 9.0\n",
      "Episode 80 with reward = 18.0\n",
      "Episode 81 with reward = 16.0\n",
      "Episode 82 with reward = 30.0\n",
      "Episode 83 with reward = 26.0\n",
      "Episode 84 with reward = 26.0\n",
      "Episode 85 with reward = 28.0\n",
      "Episode 86 with reward = 16.0\n",
      "Episode 87 with reward = 13.0\n",
      "Episode 88 with reward = 19.0\n",
      "Episode 89 with reward = 27.0\n",
      "Episode 90 with reward = 16.0\n",
      "Episode 91 with reward = 13.0\n",
      "Episode 92 with reward = 41.0\n",
      "Episode 93 with reward = 42.0\n",
      "Episode 94 with reward = 15.0\n",
      "Episode 95 with reward = 27.0\n",
      "Episode 96 with reward = 16.0\n",
      "Episode 97 with reward = 25.0\n",
      "Episode 98 with reward = 23.0\n",
      "Episode 99 with reward = 35.0\n",
      "Test Result = 16.3\n",
      "Episode 100 with reward = 18.0\n",
      "Episode 101 with reward = 14.0\n",
      "Episode 102 with reward = 68.0\n",
      "Episode 103 with reward = 12.0\n",
      "Episode 104 with reward = 23.0\n",
      "Episode 105 with reward = 34.0\n",
      "Episode 106 with reward = 22.0\n",
      "Episode 107 with reward = 13.0\n",
      "Episode 108 with reward = 14.0\n",
      "Episode 109 with reward = 17.0\n",
      "Episode 110 with reward = 15.0\n",
      "Episode 111 with reward = 20.0\n",
      "Episode 112 with reward = 13.0\n",
      "Episode 113 with reward = 21.0\n",
      "Episode 114 with reward = 12.0\n",
      "Episode 115 with reward = 16.0\n",
      "Episode 116 with reward = 43.0\n",
      "Episode 117 with reward = 20.0\n",
      "Episode 118 with reward = 19.0\n",
      "Episode 119 with reward = 15.0\n",
      "Episode 120 with reward = 14.0\n",
      "Episode 121 with reward = 25.0\n",
      "Episode 122 with reward = 22.0\n",
      "Episode 123 with reward = 18.0\n",
      "Episode 124 with reward = 15.0\n",
      "Test Result = 21.27\n",
      "Episode 125 with reward = 17.0\n",
      "Episode 126 with reward = 24.0\n",
      "Episode 127 with reward = 25.0\n",
      "Episode 128 with reward = 12.0\n",
      "Episode 129 with reward = 28.0\n",
      "Episode 130 with reward = 22.0\n",
      "Episode 131 with reward = 31.0\n",
      "Episode 132 with reward = 17.0\n",
      "Episode 133 with reward = 21.0\n",
      "Episode 134 with reward = 29.0\n",
      "Episode 135 with reward = 16.0\n",
      "Episode 136 with reward = 35.0\n",
      "Episode 137 with reward = 15.0\n",
      "Episode 138 with reward = 68.0\n",
      "Episode 139 with reward = 16.0\n",
      "Episode 140 with reward = 10.0\n",
      "Episode 141 with reward = 34.0\n",
      "Episode 142 with reward = 28.0\n",
      "Episode 143 with reward = 38.0\n",
      "Episode 144 with reward = 41.0\n",
      "Episode 145 with reward = 39.0\n",
      "Episode 146 with reward = 24.0\n",
      "Episode 147 with reward = 35.0\n",
      "Episode 148 with reward = 22.0\n",
      "Episode 149 with reward = 64.0\n",
      "Test Result = 27.95\n",
      "Episode 150 with reward = 40.0\n",
      "Episode 151 with reward = 23.0\n",
      "Episode 152 with reward = 11.0\n",
      "Episode 153 with reward = 16.0\n",
      "Episode 154 with reward = 56.0\n",
      "Episode 155 with reward = 46.0\n",
      "Episode 156 with reward = 14.0\n",
      "Episode 157 with reward = 10.0\n",
      "Episode 158 with reward = 33.0\n",
      "Episode 159 with reward = 67.0\n",
      "Episode 160 with reward = 53.0\n",
      "Episode 161 with reward = 25.0\n",
      "Episode 162 with reward = 12.0\n",
      "Episode 163 with reward = 22.0\n",
      "Episode 164 with reward = 12.0\n",
      "Episode 165 with reward = 12.0\n",
      "Episode 166 with reward = 45.0\n",
      "Episode 167 with reward = 35.0\n",
      "Episode 168 with reward = 28.0\n",
      "Episode 169 with reward = 10.0\n",
      "Episode 170 with reward = 48.0\n",
      "Episode 171 with reward = 25.0\n",
      "Episode 172 with reward = 31.0\n",
      "Episode 173 with reward = 60.0\n",
      "Episode 174 with reward = 59.0\n",
      "Test Result = 33.92\n",
      "Episode 175 with reward = 19.0\n",
      "Episode 176 with reward = 10.0\n",
      "Episode 177 with reward = 27.0\n",
      "Episode 178 with reward = 27.0\n",
      "Episode 179 with reward = 12.0\n",
      "Episode 180 with reward = 11.0\n",
      "Episode 181 with reward = 9.0\n",
      "Episode 182 with reward = 19.0\n",
      "Episode 183 with reward = 25.0\n",
      "Episode 184 with reward = 30.0\n",
      "Episode 185 with reward = 31.0\n",
      "Episode 186 with reward = 13.0\n",
      "Episode 187 with reward = 26.0\n",
      "Episode 188 with reward = 39.0\n",
      "Episode 189 with reward = 14.0\n",
      "Episode 190 with reward = 44.0\n",
      "Episode 191 with reward = 92.0\n",
      "Episode 192 with reward = 39.0\n",
      "Episode 193 with reward = 57.0\n",
      "Episode 194 with reward = 37.0\n",
      "Episode 195 with reward = 57.0\n",
      "Episode 196 with reward = 24.0\n",
      "Episode 197 with reward = 59.0\n",
      "Episode 198 with reward = 39.0\n",
      "Episode 199 with reward = 44.0\n",
      "Test Result = 40.93\n",
      "Episode 200 with reward = 26.0\n",
      "Episode 201 with reward = 38.0\n",
      "Episode 202 with reward = 59.0\n",
      "Episode 203 with reward = 10.0\n",
      "Episode 204 with reward = 20.0\n",
      "Episode 205 with reward = 66.0\n",
      "Episode 206 with reward = 43.0\n",
      "Episode 207 with reward = 17.0\n",
      "Episode 208 with reward = 44.0\n",
      "Episode 209 with reward = 21.0\n",
      "Episode 210 with reward = 28.0\n",
      "Episode 211 with reward = 63.0\n",
      "Episode 212 with reward = 63.0\n",
      "Episode 213 with reward = 45.0\n",
      "Episode 214 with reward = 44.0\n",
      "Episode 215 with reward = 58.0\n",
      "Episode 216 with reward = 82.0\n",
      "Episode 217 with reward = 24.0\n",
      "Episode 218 with reward = 30.0\n",
      "Episode 219 with reward = 27.0\n",
      "Episode 220 with reward = 44.0\n",
      "Episode 221 with reward = 19.0\n",
      "Episode 222 with reward = 15.0\n",
      "Episode 223 with reward = 51.0\n",
      "Episode 224 with reward = 30.0\n",
      "Test Result = 39.13\n",
      "Episode 225 with reward = 75.0\n",
      "Episode 226 with reward = 64.0\n",
      "Episode 227 with reward = 18.0\n",
      "Episode 228 with reward = 31.0\n",
      "Episode 229 with reward = 16.0\n",
      "Episode 230 with reward = 21.0\n",
      "Episode 231 with reward = 38.0\n",
      "Episode 232 with reward = 27.0\n",
      "Episode 233 with reward = 25.0\n",
      "Episode 234 with reward = 47.0\n",
      "Episode 235 with reward = 33.0\n",
      "Episode 236 with reward = 18.0\n",
      "Episode 237 with reward = 15.0\n",
      "Episode 238 with reward = 26.0\n",
      "Episode 239 with reward = 53.0\n",
      "Episode 240 with reward = 13.0\n",
      "Episode 241 with reward = 17.0\n",
      "Episode 242 with reward = 43.0\n",
      "Episode 243 with reward = 13.0\n",
      "Episode 244 with reward = 48.0\n",
      "Episode 245 with reward = 115.0\n",
      "Episode 246 with reward = 18.0\n",
      "Episode 247 with reward = 40.0\n",
      "Episode 248 with reward = 35.0\n",
      "Episode 249 with reward = 52.0\n",
      "Test Result = 74.07\n",
      "Episode 250 with reward = 39.0\n",
      "Episode 251 with reward = 28.0\n",
      "Episode 252 with reward = 40.0\n",
      "Episode 253 with reward = 65.0\n",
      "Episode 254 with reward = 17.0\n",
      "Episode 255 with reward = 47.0\n",
      "Episode 256 with reward = 71.0\n",
      "Episode 257 with reward = 35.0\n",
      "Episode 258 with reward = 25.0\n",
      "Episode 259 with reward = 29.0\n",
      "Episode 260 with reward = 10.0\n",
      "Episode 261 with reward = 71.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 262 with reward = 62.0\n",
      "Episode 263 with reward = 50.0\n",
      "Episode 264 with reward = 58.0\n",
      "Episode 265 with reward = 39.0\n",
      "Episode 266 with reward = 44.0\n",
      "Episode 267 with reward = 16.0\n",
      "Episode 268 with reward = 20.0\n",
      "Episode 269 with reward = 61.0\n",
      "Episode 270 with reward = 71.0\n",
      "Episode 271 with reward = 52.0\n",
      "Episode 272 with reward = 34.0\n",
      "Episode 273 with reward = 58.0\n",
      "Episode 274 with reward = 16.0\n",
      "Test Result = 87.14\n",
      "Episode 275 with reward = 27.0\n",
      "Episode 276 with reward = 64.0\n",
      "Episode 277 with reward = 51.0\n",
      "Episode 278 with reward = 19.0\n",
      "Episode 279 with reward = 121.0\n",
      "Episode 280 with reward = 111.0\n",
      "Episode 281 with reward = 26.0\n",
      "Episode 282 with reward = 30.0\n",
      "Episode 283 with reward = 23.0\n",
      "Episode 284 with reward = 33.0\n",
      "Episode 285 with reward = 128.0\n",
      "Episode 286 with reward = 67.0\n",
      "Episode 287 with reward = 82.0\n",
      "Episode 288 with reward = 21.0\n",
      "Episode 289 with reward = 96.0\n",
      "Episode 290 with reward = 119.0\n",
      "Episode 291 with reward = 73.0\n",
      "Episode 292 with reward = 101.0\n",
      "Episode 293 with reward = 34.0\n",
      "Episode 294 with reward = 9.0\n",
      "Episode 295 with reward = 83.0\n",
      "Episode 296 with reward = 45.0\n",
      "Episode 297 with reward = 127.0\n",
      "Episode 298 with reward = 73.0\n",
      "Episode 299 with reward = 30.0\n",
      "Test Result = 149.12\n",
      "Episode 0 with reward = 16.0\n",
      "Episode 1 with reward = 20.0\n",
      "Episode 2 with reward = 9.0\n",
      "Episode 3 with reward = 26.0\n",
      "Episode 4 with reward = 14.0\n",
      "Episode 5 with reward = 9.0\n",
      "Episode 6 with reward = 17.0\n",
      "Episode 7 with reward = 34.0\n",
      "Episode 8 with reward = 11.0\n",
      "Episode 9 with reward = 17.0\n",
      "Episode 10 with reward = 18.0\n",
      "Episode 11 with reward = 25.0\n",
      "Episode 12 with reward = 17.0\n",
      "Episode 13 with reward = 51.0\n",
      "Episode 14 with reward = 23.0\n",
      "Episode 15 with reward = 23.0\n",
      "Episode 16 with reward = 17.0\n",
      "Episode 17 with reward = 15.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e89d4649b1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mobs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mepisode_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_rew\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MS/Courses/Reinforcement_Learning/Project/COMP767_BQN_exploration/BayesianQNetwork.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs_t, act_t, rew_t, obs_t1, done)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MS/Courses/Reinforcement_Learning/Project/COMP767_BQN_exploration/BayesianQNetwork.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, use_sample, num_sample)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MS/Courses/Reinforcement_Learning/Project/COMP767_BQN_exploration/BayesianNetwork.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, use_sample, num_sample)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MS/Courses/Reinforcement_Learning/Project/COMP767_BQN_exploration/BayesianNetwork.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, use_sample, num_sample)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MS/Courses/Reinforcement_Learning/Project/COMP767_BQN_exploration/Distributions.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mprob1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprob2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprob1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprob2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlog_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_scale\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runs = 5\n",
    "run_result = []\n",
    "\n",
    "for run in range(runs):\n",
    "        \n",
    "    writer = SummaryWriter()\n",
    "    dbqn = BayesianNetwork(features_list, 4, batch_size, steps).to(DEVICE)    \n",
    "    target_dbqn = BayesianNetwork(features_list, 4, batch_size, steps).to(DEVICE)\n",
    "    agent = BQN_learn(dbqn, target_dbqn, gamma, lr, batch_size, buffer_size, writer)\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    episode_rew = 0\n",
    "    episode_count = 0\n",
    "    res = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    act = agent.reset(obs)         \n",
    "\n",
    "    while agent.t <= steps and episode_count < 300:\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode \" + str(episode_count) + \" with reward = \" + str(episode_rew))  \n",
    "            writer.add_scalar('data/reward', episode_rew, episode_count)\n",
    "            res.append(episode_rew)\n",
    "            episode_rew = 0\n",
    "            episode_count = episode_count + 1                            \n",
    "\n",
    "            if episode_count%25 == 0:\n",
    "                test_result = test_agent(agent)\n",
    "                print(\"Test Result = \" + str(test_result))\n",
    "                writer.add_scalar('data/test_reward', test_result, episode_count)\n",
    "                \n",
    "#             for param_group in agent.optimizer.param_groups:\n",
    "#                 if param_group['lr'] > 1e-3:\n",
    "#                     param_group['lr'] = 1e-2 - 1e-3*(episode_count//100)\n",
    "\n",
    "            obs = env.reset()\n",
    "            act = agent.reset(obs)   \n",
    "\n",
    "        obs1, rew, done, _ = env.step(act)       \n",
    "        act = agent.step(obs, act, rew, obs1, done)\n",
    "        obs = obs1\n",
    "        episode_rew = episode_rew + rew    \n",
    "            \n",
    "    writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "    writer.close()\n",
    "        \n",
    "    run_result.append(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
